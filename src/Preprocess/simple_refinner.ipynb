{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\belfo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.7.0)\n",
      "Requirement already satisfied: clean-text[gpl] in c:\\users\\belfo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\belfo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text[gpl]) (6.3.1)\n",
      "Requirement already satisfied: unidecode<2.0.0,>=1.1.1 in c:\\users\\belfo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clean-text[gpl]) (1.3.8)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\belfo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ftfy<7.0,>=6.0->clean-text[gpl]) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "! pip install clean-text[gpl] emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\belfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Baixar stopwords do NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Função de pré-processamento para issues com remoção de código\n",
    "def preprocess_issue(text):\n",
    "    # Remoção de HTML ou tags personalizadas\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Substituição de blocos de código Markdown (``` ou ~~~) por <CODE_BLOCK>\n",
    "    text = re.sub(r'(```.*?```|~~~.*?~~~)', '<CODE_BLOCK>', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Substituição de código inline (delimitado por `) por <CODE_BLOCK>\n",
    "    text = re.sub(r'`[^`]+`', '<CODE_BLOCK>', text)\n",
    "    \n",
    "    # Substituição de referências a issues ou PRs (ex: #123) por <ISSUE_REF>\n",
    "    text = re.sub(r'#\\d+', '<ISSUE_REF>', text)\n",
    "    \n",
    "    # Substituição de menções (@username) por <MENTION>\n",
    "    text = re.sub(r'@\\w+', '<MENTION>', text)\n",
    "    \n",
    "    # Substituição de links por <LINK>\n",
    "    text = re.sub(r'http[s]?://\\S+', '<LINK>', text)\n",
    "    \n",
    "    # Substituição de e-mails por <EMAIL>\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', text)\n",
    "    \n",
    "    # Substituição de grandes números ou IDs por <CODE>\n",
    "    text = re.sub(r'\\b\\d{4,}\\b', '<CODE>', text)\n",
    "\n",
    "    # Substituição de numeros por <NUMBER>\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUMBER>', text)\n",
    "    \n",
    "    # Remoção de formatações Markdown (ex: **bold**, _italic_)\n",
    "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text)  # Remove ** ou __\n",
    "    text = re.sub(r'(_)(.*?)\\1', r'\\2', text)  # Remove _\n",
    "    \n",
    "    # Remoção de pontuações, preservando tokens como <LINK>\n",
    "    text = re.sub(r'[^\\w\\s<>]', '', text)\n",
    "    \n",
    "    # Remoção de stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    text = ' '.join(filtered_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing oasisprotocol/oasis-core...\n",
      "Processing klaytn/klaytn...\n",
      "Processing harmony-one/harmony...\n",
      "Processing parallel-finance/parallel...\n",
      "Processing freeverseio/laos...\n",
      "Processing nucypher/nucypher...\n",
      "Processing ethereum/go-ethereum...\n",
      "Processing witnet/witnet-rust...\n",
      "Processing sora-xor/sora2-network...\n",
      "Processing Agoric/agoric-sdk...\n",
      "Processing ainblockchain/ain-blockchain...\n",
      "Processing bigbangcore/BigBang...\n",
      "Processing massalabs/massa...\n",
      "Processing ton-blockchain/TEPs...\n",
      "Processing red/red...\n",
      "Processing cryptoblades/cryptoblades...\n",
      "Processing Veil-Project/veil...\n",
      "Processing spartan-protocol/SpartanProtocol-DAppV2...\n",
      "Processing celo-org/celo-monorepo...\n",
      "Processing OriginProtocol/origin-dollar...\n",
      "Processing starcoinorg/starcoin...\n",
      "Processing KlimaDAO/klimadao...\n",
      "Processing peercoin/peercoin...\n",
      "Processing DeFiCh/ain...\n",
      "Processing open-chat-labs/open-chat...\n",
      "Processing PIVX-Project/PIVX...\n",
      "Processing steemit/steem...\n",
      "Processing omgnetwork/elixir-omg...\n",
      "Processing trustwallet/assets...\n",
      "Processing mobilecoinfoundation/mobilecoin...\n",
      "Processing hyle-team/zano...\n",
      "Processing syscoin/syscoin...\n",
      "Processing gnosis/cowswap...\n",
      "Processing EOSIO/eos...\n",
      "Processing liquity/dev...\n",
      "Processing scrtlabs/SecretNetwork...\n",
      "Processing storj/storj...\n",
      "Processing Constellation-Labs/constellation...\n",
      "Processing metaplex-foundation/metaplex...\n",
      "Processing Chia-Network/chia-blockchain...\n",
      "Processing ssvlabs/ssv...\n",
      "Processing MetaMask/metamask-extension...\n",
      "Processing DimensionDev/Maskbook...\n",
      "Processing RavenProject/Ravencoin...\n",
      "Processing qtumproject/qtum...\n",
      "Processing osmosis-labs/osmosis...\n",
      "Processing OriginTrail/ot-node...\n",
      "Processing iotexproject/iotex-core...\n",
      "Processing dashpay/dash...\n",
      "Processing brave-intl/bat-ledger...\n",
      "Processing Zilliqa/Zilliqa...\n",
      "Processing holochain/holochain-rust...\n",
      "Processing paritytech/polkadot-sdk...\n",
      "Processing AstarNetwork/Astar...\n",
      "Processing BTCGPU/BTCGPU...\n",
      "Processing wormhole-foundation/wormhole...\n",
      "Processing zcash/zcash...\n",
      "Processing MinaProtocol/mina...\n",
      "Processing neo-project/neo...\n",
      "Processing BeamMW/beam...\n",
      "Processing AntelopeIO/leap...\n",
      "Processing bitcoin-sv/bitcoin-sv...\n",
      "Processing ArweaveTeam/arweave...\n",
      "Processing monero-project/monero...\n",
      "Processing opentensor/BitTensor...\n",
      "Processing ethereumclassic/ethereumclassic.github.io...\n",
      "Processing litecoin-project/litecoin...\n",
      "Processing smartcontractkit/chainlink...\n",
      "Processing stellar/stellar-core...\n",
      "Processing tronprotocol/java-tron...\n",
      "Processing ava-labs/avalanchego...\n",
      "Processing dogecoin/dogecoin...\n",
      "Processing bitcoin/bitcoin...\n",
      "Refined data saved to refined_issues.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the previously saved data\n",
    "with open(\"raw_issues.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process each repository's issues and their comments\n",
    "refined_data = []\n",
    "for repo, issues in data.items():\n",
    "    print(f\"Processing {repo}...\")\n",
    "    for issue in issues:\n",
    "        # Preprocess the issue fields\n",
    "        refined_issue = {\n",
    "            \"repo\": repo,\n",
    "            \"createdAt\": issue[\"createdAt\"],\n",
    "            \"title\": preprocess_issue(issue[\"title\"]),\n",
    "            \"body\": preprocess_issue(issue[\"body\"]),\n",
    "            \"comments\": []\n",
    "        }\n",
    "\n",
    "        # Preprocess the comments\n",
    "        for comment in issue.get(\"comments\", []):\n",
    "            refined_issue[\"comments\"].append({\n",
    "                \"body\": preprocess_issue(comment[\"body\"]),\n",
    "                \"createdAt\": comment[\"createdAt\"]\n",
    "            })\n",
    "\n",
    "        refined_data.append(refined_issue)\n",
    "\n",
    "# Save the refined data back to JSON, preserving order\n",
    "with open(\"refined_issues.json\", \"w\") as file:\n",
    "    json.dump(refined_data, file, indent=4)\n",
    "\n",
    "print(\"Refined data saved to refined_issues.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
